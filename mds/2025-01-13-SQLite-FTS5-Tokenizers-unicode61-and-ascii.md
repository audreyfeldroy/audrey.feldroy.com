# SQLite FTS5 Tokenizers: `unicode61` and `ascii`

The SQLite FTS5 (full-text search) extension includes the built-in tokenizers unicode61, ascii, porter, and trigram, implemented in [fts5_tokenize.c](https://github.com/sqlite/sqlite/blob/master/ext/fts5/fts5_tokenize.c). [APSW](https://github.com/rogerbinns/apsw) provides a Python API to use those. 

Here we explore the default tokenizer `unicode61` and built-in tokenizer `ascii` in detail.

## Overview

When you use SQLite's FTS5 extension, you create a FTS5 virtual table and specify which of its tokenizers to use (or a custom one of your own).

The tokenizer defines how text is split into tokens, and how it'll be matched in searches.

## Setup


```python
import apsw
import apsw.ext
```


```python
connection = apsw.Connection("dbfile")
connection
```




    <apsw.Connection at 0x10dfb6e30>



## The Default SQLite FTS5 Tokenizer `unicode61`

This is implemented in SQLite [fts5_tokenize.c starting at line 175](https://github.com/sqlite/sqlite/blob/master/ext/fts5/fts5_tokenize.c#L175) with Python wrapper in APSW [fts5.py](https://github.com/rogerbinns/apsw/blob/master/apsw/fts5.py).


```python
tokenizer = connection.fts5_tokenizer("unicode61")
tokenizer
```




    <apsw.FTS5Tokenizer at 0x10df61c00>



I like the [apsw tokenizer docs'](https://rogerbinns.github.io/apsw/example-fts.html#tokenizers) test text:


```python
txt = """ğŸ¤¦ğŸ¼â€â™‚ï¸ v1.2 Grey â…¢ ColOUR! Don't jump -  ğŸ‡«ğŸ‡®ä½ å¥½ä¸–ç•Œ StraÃŸe
    à¤¹à¥ˆà¤²à¥‹ à¤µà¤°à¥à¤²à¥à¤¡ DÃ©jÃ  vu RÃ©sumÃ© SQLITE_ERROR"""
```


```python
encoded = txt.encode("utf8")
encoded
```




    b"\xf0\x9f\xa4\xa6\xf0\x9f\x8f\xbc\xe2\x80\x8d\xe2\x99\x82\xef\xb8\x8f v1.2 Grey \xe2\x85\xa2 ColOUR! Don't jump -  \xf0\x9f\x87\xab\xf0\x9f\x87\xae\xe4\xbd\xa0\xe5\xa5\xbd\xe4\xb8\x96\xe7\x95\x8c Stra\xc3\x9fe\n    \xe0\xa4\xb9\xe0\xa5\x88\xe0\xa4\xb2\xe0\xa5\x8b \xe0\xa4\xb5\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xb2\xe0\xa5\x8d\xe0\xa4\xa1 D\xc3\xa9j\xc3\xa0 vu R\xc3\xa9sum\xc3\xa9 SQLITE_ERROR"



Yeah, remember `txt` ends in `SQLITE_ERROR`, that's expected. Interesting that it doesn't get encoded by `fts5_tokenizer`.


```python
tokenized = tokenizer(encoded, apsw.FTS5_TOKENIZE_DOCUMENT, None, include_offsets=False)
tokenized
```




    [('ğŸ¤¦ğŸ¼',),
     ('v1',),
     ('2',),
     ('grey',),
     ('â…²',),
     ('colour',),
     ('don',),
     ('t',),
     ('jump',),
     ('ä½ å¥½ä¸–ç•Œ',),
     ('straÃŸe',),
     ('à¤¹',),
     ('à¤²',),
     ('à¤µà¤°',),
     ('à¤²',),
     ('à¤¡',),
     ('deja',),
     ('vu',),
     ('resume',),
     ('sqlite',),
     ('error',)]



Observations about tokenization with `unicode61`:

| Observation | Example |
|------------|---------|
| Emojis are preserved as single tokens | `ğŸ¤¦ğŸ¼` |
| Numbers are separated on decimal points | `v1.2` â†’ `v1`, `2` |
| Case is normalized to lowercase | `ColOUR` â†’ `colour` |
| Single Roman numeral Unicode characters are preserved | `â…¢` â†’ `â…²` |
| Apostrophes split words | `Don't` â†’ `don`, `t` |
| Hyphens and spaces are removed | `jump -  ğŸ‡«ğŸ‡®ä½ å¥½ä¸–ç•Œ` â†’ `jump`, `ä½ å¥½ä¸–ç•Œ` |
| Chinese characters stay together (Google Translate says `ä½ å¥½ä¸–ç•Œ` is "Hello World") | `ä½ å¥½ä¸–ç•Œ` kept as one token |
| German eszett (ÃŸ) is preserved | `StraÃŸe` â†’ `straÃŸe` |
| Hindi written in Devanagari script is split into parts (Google Translate detects `à¤¹à¥ˆà¤²à¥‹ à¤µà¤°à¥à¤²à¥à¤¡` as Hindi for "Hello World": `à¤¹à¥ˆ` "is", ) | `à¤¹à¥ˆà¤²à¥‹` â†’ `à¤¹`, `à¤²` |
| Diacritics are removed | `DÃ©jÃ ` â†’ `deja` |
| Underscores split words | `SQLITE_ERROR` â†’ `sqlite`, `error` |
| Emojis as part of a word are ignored (?) | `ğŸ‡«ğŸ‡®` in `ğŸ‡«ğŸ‡®ä½ å¥½ä¸–ç•Œ` not tokenized |

## Included SQLite FTS5 Tokenizer `ascii`

This is implemented in SQLite [fts5_tokenize.c starting at line 18](https://github.com/sqlite/sqlite/blob/master/ext/fts5/fts5_tokenize.c#L18) with Python wrapper in APSW [fts5.py](https://github.com/rogerbinns/apsw/blob/master/apsw/fts5.py).


```python
tokenizer_ascii = connection.fts5_tokenizer("ascii")
tokenizer_ascii
```




    <apsw.FTS5Tokenizer at 0x10de5aec0>




```python
tokenized = tokenizer_ascii(encoded, apsw.FTS5_TOKENIZE_DOCUMENT, None, include_offsets=False)
tokenized
```




    [('ğŸ¤¦ğŸ¼\u200dâ™‚ï¸',),
     ('v1',),
     ('2',),
     ('grey',),
     ('â…¢',),
     ('colour',),
     ('don',),
     ('t',),
     ('jump',),
     ('ğŸ‡«ğŸ‡®ä½ å¥½ä¸–ç•Œ',),
     ('straÃŸe',),
     ('à¤¹à¥ˆà¤²à¥‹',),
     ('à¤µà¤°à¥à¤²à¥à¤¡',),
     ('dÃ©jÃ ',),
     ('vu',),
     ('rÃ©sumÃ©',),
     ('sqlite',),
     ('error',)]



Comparing our `ascii` tokenization here with the above `unicode61` tokenization:

| Feature | unicode61 | ascii | Example |
|---------|-----------|--------|---------|
| Emoji handling | Preserves simple emojis | Keeps composite emojis with modifiers | `ğŸ¤¦ğŸ¼` vs `ğŸ¤¦ğŸ¼\u200dâ™‚ï¸` |
| Number splitting | Splits on decimal points | Same | `v1.2` â†’ `v1`, `2` |
| Case normalization | Converts to lowercase | Same | `ColOUR` â†’ `colour` |
| Roman numeral unicode characters | Preserves and lowercases as equivalent Unicode character | Preserves original Unicode character | `â…¢` â†’ `â…²` vs `â…¢` |
| Apostrophes | Splits words | Same | `Don't` â†’ `don`, `t` |
| Spaces/hyphens | Removes | Same | `jump -` â†’ `jump` |
| CJK characters | Keeps together | Same | `ä½ å¥½ä¸–ç•Œ` stays as one token |
| German eszett | Preserves | Same | `StraÃŸe` â†’ `straÃŸe` |
| Devanagari script | Splits into parts | Keeps words together | `à¤¹à¥ˆà¤²à¥‹` â†’ `à¤¹`, `à¤²` vs `à¤¹à¥ˆà¤²à¥‹` |
| Diacritics | Removes | Preserves | `DÃ©jÃ ` â†’ `deja` vs `dÃ©jÃ ` |
| Underscores | Splits words | Same | `SQLITE_ERROR` â†’ `sqlite`, `error` |
| Emoji as part of a word | Ignores | Keeps with following text | `ğŸ‡«ğŸ‡®ä½ å¥½ä¸–ç•Œ` |

## Choosing a Tokenizer

There's no best tokenizer: choose based on your use case. Here I'm considering the use case of implementing search for a blog or simple personal website, such as this one.

Since I write my notebooks in English and don't really use emoji, the default of `unicode61` is fine.

## Search

To be continued...

## Credits

Thanks to [Daniel Roy Greenfeld](https://daniel.feldroy.com) for reviewing.
