{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccec3650",
   "metadata": {},
   "source": [
    "# SQLite FTS5 Tokenizers: `unicode61` and `ascii`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755860d",
   "metadata": {},
   "source": [
    "The SQLite FTS5 (full-text search) extension includes the built-in tokenizers unicode61, ascii, porter, and trigram, implemented in [fts5_tokenize.c](https://github.com/sqlite/sqlite/blob/master/ext/fts5/fts5_tokenize.c). [APSW](https://github.com/rogerbinns/apsw) provides a Python API to use those. \n",
    "\n",
    "Here we explore the default tokenizer `unicode61` and built-in tokenizer `ascii` in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8768a78",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98b815",
   "metadata": {},
   "source": [
    "When you use SQLite's FTS5 extension, you create a FTS5 virtual table and specify which of its tokenizers to use (or a custom one of your own).\n",
    "\n",
    "The tokenizer defines how text is split into tokens, and how it'll be matched in searches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1131a38",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10deb0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apsw\n",
    "import apsw.ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef96bede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apsw.Connection at 0x10dfb6e30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection = apsw.Connection(\"dbfile\")\n",
    "connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9b706",
   "metadata": {},
   "source": [
    "## The Default SQLite FTS5 Tokenizer `unicode61`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d482235",
   "metadata": {},
   "source": [
    "This is implemented in SQLite [fts5_tokenize.c starting at line 175](https://github.com/sqlite/sqlite/blob/master/ext/fts5/fts5_tokenize.c#L175) with Python wrapper in APSW [fts5.py](https://github.com/rogerbinns/apsw/blob/master/apsw/fts5.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27f4686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apsw.FTS5Tokenizer at 0x10df61c00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = connection.fts5_tokenizer(\"unicode61\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763978e",
   "metadata": {},
   "source": [
    "I like the [apsw tokenizer docs'](https://rogerbinns.github.io/apsw/example-fts.html#tokenizers) test text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e81354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"ü§¶üèº‚Äç‚ôÇÔ∏è v1.2 Grey ‚Ö¢ ColOUR! Don't jump -  üá´üáÆ‰Ω†Â•Ω‰∏ñÁïå Stra√üe\n",
    "    ‡§π‡•à‡§≤‡•ã ‡§µ‡§∞‡•ç‡§≤‡•ç‡§° D√©j√† vu R√©sum√© SQLITE_ERROR\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d4dcdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"\\xf0\\x9f\\xa4\\xa6\\xf0\\x9f\\x8f\\xbc\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f v1.2 Grey \\xe2\\x85\\xa2 ColOUR! Don't jump -  \\xf0\\x9f\\x87\\xab\\xf0\\x9f\\x87\\xae\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd\\xe4\\xb8\\x96\\xe7\\x95\\x8c Stra\\xc3\\x9fe\\n    \\xe0\\xa4\\xb9\\xe0\\xa5\\x88\\xe0\\xa4\\xb2\\xe0\\xa5\\x8b \\xe0\\xa4\\xb5\\xe0\\xa4\\xb0\\xe0\\xa5\\x8d\\xe0\\xa4\\xb2\\xe0\\xa5\\x8d\\xe0\\xa4\\xa1 D\\xc3\\xa9j\\xc3\\xa0 vu R\\xc3\\xa9sum\\xc3\\xa9 SQLITE_ERROR\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = txt.encode(\"utf8\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763bef5",
   "metadata": {},
   "source": [
    "Yeah, remember `txt` ends in `SQLITE_ERROR`, that's expected. Interesting that it doesn't get encoded by `fts5_tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14dac26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ü§¶üèº',),\n",
       " ('v1',),\n",
       " ('2',),\n",
       " ('grey',),\n",
       " ('‚Ö≤',),\n",
       " ('colour',),\n",
       " ('don',),\n",
       " ('t',),\n",
       " ('jump',),\n",
       " ('‰Ω†Â•Ω‰∏ñÁïå',),\n",
       " ('stra√üe',),\n",
       " ('‡§π',),\n",
       " ('‡§≤',),\n",
       " ('‡§µ‡§∞',),\n",
       " ('‡§≤',),\n",
       " ('‡§°',),\n",
       " ('deja',),\n",
       " ('vu',),\n",
       " ('resume',),\n",
       " ('sqlite',),\n",
       " ('error',)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer(encoded, apsw.FTS5_TOKENIZE_DOCUMENT, None, include_offsets=False)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a285820",
   "metadata": {},
   "source": [
    "Observations about tokenization with `unicode61`:\n",
    "\n",
    "| Observation | Example |\n",
    "|------------|---------|\n",
    "| Emojis are preserved as single tokens | `ü§¶üèº` |\n",
    "| Numbers are separated on decimal points | `v1.2` ‚Üí `v1`, `2` |\n",
    "| Case is normalized to lowercase | `ColOUR` ‚Üí `colour` |\n",
    "| Single Roman numeral Unicode characters are preserved | `‚Ö¢` ‚Üí `‚Ö≤` |\n",
    "| Apostrophes split words | `Don't` ‚Üí `don`, `t` |\n",
    "| Hyphens and spaces are removed | `jump -  üá´üáÆ‰Ω†Â•Ω‰∏ñÁïå` ‚Üí `jump`, `‰Ω†Â•Ω‰∏ñÁïå` |\n",
    "| Chinese characters stay together (Google Translate says `‰Ω†Â•Ω‰∏ñÁïå` is \"Hello World\") | `‰Ω†Â•Ω‰∏ñÁïå` kept as one token |\n",
    "| German eszett (√ü) is preserved | `Stra√üe` ‚Üí `stra√üe` |\n",
    "| Hindi written in Devanagari script is split into parts (Google Translate detects `‡§π‡•à‡§≤‡•ã ‡§µ‡§∞‡•ç‡§≤‡•ç‡§°` as Hindi for \"Hello World\": `‡§π‡•à` \"is\", ) | `‡§π‡•à‡§≤‡•ã` ‚Üí `‡§π`, `‡§≤` |\n",
    "| Diacritics are removed | `D√©j√†` ‚Üí `deja` |\n",
    "| Underscores split words | `SQLITE_ERROR` ‚Üí `sqlite`, `error` |\n",
    "| Emojis as part of a word are ignored (?) | `üá´üáÆ` in `üá´üáÆ‰Ω†Â•Ω‰∏ñÁïå` not tokenized |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166170b",
   "metadata": {},
   "source": [
    "## Included SQLite FTS5 Tokenizer `ascii`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e163bc",
   "metadata": {},
   "source": [
    "This is implemented in SQLite [fts5_tokenize.c starting at line 18](https://github.com/sqlite/sqlite/blob/master/ext/fts5/fts5_tokenize.c#L18) with Python wrapper in APSW [fts5.py](https://github.com/rogerbinns/apsw/blob/master/apsw/fts5.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a438b102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apsw.FTS5Tokenizer at 0x10de5aec0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ascii = connection.fts5_tokenizer(\"ascii\")\n",
    "tokenizer_ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f49c182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ü§¶üèº\\u200d‚ôÇÔ∏è',),\n",
       " ('v1',),\n",
       " ('2',),\n",
       " ('grey',),\n",
       " ('‚Ö¢',),\n",
       " ('colour',),\n",
       " ('don',),\n",
       " ('t',),\n",
       " ('jump',),\n",
       " ('üá´üáÆ‰Ω†Â•Ω‰∏ñÁïå',),\n",
       " ('stra√üe',),\n",
       " ('‡§π‡•à‡§≤‡•ã',),\n",
       " ('‡§µ‡§∞‡•ç‡§≤‡•ç‡§°',),\n",
       " ('d√©j√†',),\n",
       " ('vu',),\n",
       " ('r√©sum√©',),\n",
       " ('sqlite',),\n",
       " ('error',)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer_ascii(encoded, apsw.FTS5_TOKENIZE_DOCUMENT, None, include_offsets=False)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4b44c",
   "metadata": {},
   "source": [
    "Comparing our `ascii` tokenization here with the above `unicode61` tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542db30",
   "metadata": {},
   "source": [
    "| Feature | unicode61 | ascii | Example |\n",
    "|---------|-----------|--------|---------|\n",
    "| Emoji handling | Preserves simple emojis | Keeps composite emojis with modifiers | `ü§¶üèº` vs `ü§¶üèº\\u200d‚ôÇÔ∏è` |\n",
    "| Number splitting | Splits on decimal points | Same | `v1.2` ‚Üí `v1`, `2` |\n",
    "| Case normalization | Converts to lowercase | Same | `ColOUR` ‚Üí `colour` |\n",
    "| Roman numeral unicode characters | Preserves and lowercases as equivalent Unicode character | Preserves original Unicode character | `‚Ö¢` ‚Üí `‚Ö≤` vs `‚Ö¢` |\n",
    "| Apostrophes | Splits words | Same | `Don't` ‚Üí `don`, `t` |\n",
    "| Spaces/hyphens | Removes | Same | `jump -` ‚Üí `jump` |\n",
    "| CJK characters | Keeps together | Same | `‰Ω†Â•Ω‰∏ñÁïå` stays as one token |\n",
    "| German eszett | Preserves | Same | `Stra√üe` ‚Üí `stra√üe` |\n",
    "| Devanagari script | Splits into parts | Keeps words together | `‡§π‡•à‡§≤‡•ã` ‚Üí `‡§π`, `‡§≤` vs `‡§π‡•à‡§≤‡•ã` |\n",
    "| Diacritics | Removes | Preserves | `D√©j√†` ‚Üí `deja` vs `d√©j√†` |\n",
    "| Underscores | Splits words | Same | `SQLITE_ERROR` ‚Üí `sqlite`, `error` |\n",
    "| Emoji as part of a word | Ignores | Keeps with following text | `üá´üáÆ‰Ω†Â•Ω‰∏ñÁïå` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71688365",
   "metadata": {},
   "source": [
    "## Choosing a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1535bb3",
   "metadata": {},
   "source": [
    "There's no best tokenizer: choose based on your use case. Here I'm considering the use case of implementing search for a blog or simple personal website, such as this one.\n",
    "\n",
    "Since I write my notebooks in English and don't really use emoji, the default of `unicode61` is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b7bd1",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1e48c",
   "metadata": {},
   "source": [
    "To be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ba53a",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba1221",
   "metadata": {},
   "source": [
    "Thanks to [Daniel Roy Greenfeld](https://daniel.feldroy.com) for reviewing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
